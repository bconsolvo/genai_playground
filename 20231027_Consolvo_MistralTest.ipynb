{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c3b4195-e66e-41b8-8174-de11de0e62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import random\n",
    "import os\n",
    "# from vllm import SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37fea8e-3070-4f5c-ab63-12dd8fbc76d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f325251a-f4d8-45d3-bba1-db0088a91bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.35.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e33161-0c05-41cf-bca6-14e3f6377939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da003327-4b81-44dc-95dc-bc680762ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "if torch.xpu.is_available():\n",
    "    seed = 88\n",
    "    random.seed(seed)\n",
    "    torch.xpu.manual_seed(seed)\n",
    "    torch.xpu.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b04890b4-3c70-4bd4-90e2-36d9185a405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf = \"mistralai/Mistral-7B-v0.1\"\n",
    "torch_dtype = torch.bfloat16\n",
    "device = torch.device(\"xpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad0b5c7-402f-47f0-b10f-924177df1e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cbc7752147400d9efd8dd5130eb3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_hf)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6849a532-037c-4b8e-8ee8-50952175549e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0b2cb-fad0-4a8b-81e8-7bfbcc8cc665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ffdff4-381c-4673-a735-16fb646529ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79324cc910040189a48775f7362c7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_hf)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_hf,\n",
    "    low_cpu_mem_usage = True,\n",
    "    torch_dtype = torch_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a584b4-a310-470f-a7cb-8d0034bcb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03097fcb-7977-442f-be2a-9e06f06f26a0",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "To the extent that any public or non-Intel datasets or models are referenced by or accessed using these materials those datasets or models are provided by the third party indicated as the content source. Intel does not create the content and does not warrant its accuracy or quality. By accessing the public content, or using materials trained on or with such content, you agree to the terms associated with that content and that your use complies with the applicable license.\r\n",
    "\r\n",
    "Intel expressly disclaims the accuracy, adequacy, or completeness of any such public content, and is not liable for any errors, omissions, or defects in the content, or for any reliance on the content. Intel is not liable for any liability or damages relating to your use of public content.\r\n",
    "\r\n",
    "Intel’s provision of these resources does not expand or otherwise alter Intel’s applicable published warranties or warranty disclaimers for Intel products or solutions, and no additional obligations, indemnifications, or liabilities arise from Intel providing such resources. Intel reserves the right, without notice, to make corrections, enhancements, improvements, and other changes to its materials.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea35717-dc97-4aaa-bab4-a18d1af40f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
