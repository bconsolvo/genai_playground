{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5436d369-300d-48a5-9c3c-c717102e001e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2830e50e02419784b15e8f43439a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import os\n",
    "import transformers\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "model_hf = \"mistralai/Mistral-7B-v0.1\"\n",
    "device = torch.device(\"xpu\") # the device to load the model onto\n",
    "torch_dtype = torch.bfloat16\n",
    "xpu_autocast = torch.xpu.amp.autocast\n",
    "cpu_autocast = torch.cpu.amp.autocast\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = (\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-v0.1\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch_dtype\n",
    "    ).to(device).eval()\n",
    ")\n",
    "\n",
    "model = ipex.optimize(model, dtype=torch_dtype)\n",
    "\n",
    "# chat_template = [\n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "\n",
    "# encodeds = tokenizer.apply_chat_template(chat_template, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# model_inputs = encodeds.to(device)\n",
    "\n",
    "# with xpu_autocast(enabled=True,dtype=torch_dtype):\n",
    "#     with torch.no_grad():\n",
    "#         generated_ids = model.generate(\n",
    "#             model_inputs, \n",
    "#             max_new_tokens=1000, \n",
    "#             do_sample=True\n",
    "#         )\n",
    "# decoded = tokenizer.batch_decode(generated_ids)\n",
    "# print(decoded[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89b812c7-969a-4730-bed0-f72e96e978bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Giraffes are the tallest mammals in the world. They can grow up to 18 feet tall and weigh up to 3,000 pounds.\n",
      "\n",
      "Gir\n"
     ]
    }
   ],
   "source": [
    "input_text = \"How old is a giraffe?\"\n",
    "# input_text \"Today I went to the park and \"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "with xpu_autocast(enabled=True, dtype=torch_dtype):\n",
    "    output = model.generate(input_ids, max_length=50,pad_token_id=tokenizer.eos_token_id)\n",
    "# print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "print(tokenizer.batch_decode(output[:, input_ids.shape[1]:],skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e1e23ab-e09b-4831-ad41-9d5f857123c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad0b5c7-402f-47f0-b10f-924177df1e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_hf)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a584b4-a310-470f-a7cb-8d0034bcb1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03097fcb-7977-442f-be2a-9e06f06f26a0",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "To the extent that any public or non-Intel datasets or models are referenced by or accessed using these materials those datasets or models are provided by the third party indicated as the content source. Intel does not create the content and does not warrant its accuracy or quality. By accessing the public content, or using materials trained on or with such content, you agree to the terms associated with that content and that your use complies with the applicable license.\r\n",
    "\r\n",
    "Intel expressly disclaims the accuracy, adequacy, or completeness of any such public content, and is not liable for any errors, omissions, or defects in the content, or for any reliance on the content. Intel is not liable for any liability or damages relating to your use of public content.\r\n",
    "\r\n",
    "Intel’s provision of these resources does not expand or otherwise alter Intel’s applicable published warranties or warranty disclaimers for Intel products or solutions, and no additional obligations, indemnifications, or liabilities arise from Intel providing such resources. Intel reserves the right, without notice, to make corrections, enhancements, improvements, and other changes to its materials.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea35717-dc97-4aaa-bab4-a18d1af40f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
